


import numpy, random, os

learningrate = 0.1  # might lower to adjsut depending on the loss without sacrificing speed of training 
bias = 0 

weights = [random.random(), ] #the inputs would essenitally be the neurons

def tester (input1, input2 , input3, )


# define a helper function that implements a activation function 
# store Dot product : input1*weights[0]+input2*weights[1]+bias*weights[2] etc.

# calculate Error 
# error = output - outputP

# Calculate weights
# weights[0] += error * input1 * lr
# weights[1] += error * input2 * lr
# weights[2] += error * bias * lr

# Some loop with a batch size iterating
# over the helper function feeding it correct data. 


#input data being tested

# overwriting stored dot product with new dot product 

# activation function again this time not a helper function.


#print 
